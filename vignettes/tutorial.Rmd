---
title: "'pcSteiner' vignette"
author: "Aleksei Krasikov"
date: "`r Sys.Date()`"
output: rmarkdown::pdf_document
header-includes:
   - \usepackage{amsmath}
   - \usepackage[ruled, vlined]{algorithm2e}
   - \usepackage{hyperref}
   - \hypersetup{
         colorlinks=true,
         linkcolor=blue,
         filecolor=magenta,      
         urlcolor=blue,
     }
   - \newcommand{\forceindent}{\leavevmode{\parindent=1em\indent}}
   - \renewcommand{\thealgocf}{}
   - \usepackage{etoc}
   - \usepackage{blindtext}
vignette: >
  %\VignetteIndexEntry{tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

\vspace{15px}

\leftskip3cm\relax
\rightskip3cm\relax
This vignette document sets the following goals: expound the Prize-Collecting Steiner tree problem, give some background on Belief propagation algorithm, explain in details how BP is used for solving the Prize-Collecting Steiner tree problem, discuss several additional features for graph analysis and illustrate a typical workflow with the package.

\vspace{25px}

\leftskip0cm\relax
\rightskip0cm\relax
\tableofcontents
\etocsettocstyle{\subsection*{This Chapter contains:}}{\noindent\rule{\linewidth}{.4pt}}

\pagebreak

# 1 Steiner tree problem

\forceindent The Steiner problem is a classical combinatorial optimisation problem. The interest in it arises from a wide range of practical applications in the areas such as chemistry, biology, telecommunication and many more. For example, the goal of the Steiner tree problem in network analysis in systems biology is to detect biological relationship between a set of distinguished proteins, metabolites or genes.

\forceindent Let $G = (V, E)$ be an undirected graph, where $V$ is a vertex set and $E$ is an edge set. Suppose that graph is weighted, i.e. graph with a cost function $c: E \rightarrow R_+$. Given a required node subset $T \subseteq V$ the *Steiner tree problem* for $T$ in $G$ is to find connected, minimum cost subgraph $G_1 = (V_1, E_1)$ with $T \subseteq V_1 \subseteq V$ and $E_1 \subseteq E$. Note, that resulting subgraph is necessary a tree. The elements of $T$ are called *terminals* and elements of $V_1 \setminus T$ are *Steiner nodes*.

|    **Problem 1.1 (Steiner tree problem).**
|         *Input:* an undirected graph $G$, a set $T \subseteq V$ and weights $c:E \rightarrow R_+$
|         *Output:* a minimum weight tree connecting all vertices of $T$ in $G$

**Theorem 1.1.** The Steiner tree problem is $NP$-complete, even for unit weights (Karp 1972).

\forceindent There are two special cases of the Steiner tree problem. If $|T| = 2$, then the problem is reduced to the Shortest path problem between a pair of vertices in a graph, which is in the class $P$. The second special case to be considered is when $|T| = |V|$ and the problem is reduced to finding Minimum spanning tree, which is again solvable in polynomial time.

\forceindent In many cases nodes have an additional numerical value, which represent their significance. That is where the *Prize-Collecting Steiner tree problem* arises. Roughly speaking, the goal is to find a subgraph $G_1$ in $G$ connecting all the terminals $T$ with the most expensive nodes and least expensive edges. Note, that the result will be a tree as in the Steiner problem.

|    **Problem 1.2 (Prize-Collecting Steiner tree problem).**
|         *Input:* an undirected graph $G$, a set $T \subseteq V$, costs $c:E \rightarrow R_+$ and prizes $p: V \rightarrow R_+$
|         *Output:* find a tree $G_1$ by minimizing the following function $f(G_1) = \sum_{e \in E_1} c_e + \lambda \sum_{i \notin V_1} p_i$

Since a constant value does not change a maximum, we can subtract a total node prizes from objective function $C = \sum_{i \in V} p_i$.

|    **Problem 1.2 (revisited).**
|         *Input:* an undirected graph $G$, a set $T \subseteq V$, costs $c:E \rightarrow R_+$ and prizes $p: V \rightarrow R_+$
|         *Output:* find a tree $G_1$ by minimizing the following function $f(G_1) = \sum_{e \in E_1} c_e - \lambda \sum_{i \in V_1} p_i$

The Prize-Collecting Steiner tree problem reduces to the Steiner problem when all prizes are equal to one, so it is at least NP-complete. Thus, we do not expect to have an efficient algorithm for solving PCST problem.

*References*

1. B. Korte and J. Vygen, "Combinatorial Optimization. Theory and Algorithms". Springer, 2008.

\pagebreak

# 2 Belief propagation

\forceindent The approximation algorithm for PCST problem internally utilizes loopy belief propagation equations. In this section we remind several basic definitions in probability theory and statistics which are crucial for understanding the material presented in the later sections of this vignette document.

\forceindent For more details in implementation of belief propagation and loopy belief propagation we refer to this \href{https://github.com/krashkov/Belief-Propagation}{\underline{GitHub repository}}.

## 2.1 Preliminaries: graphical models and statistical inference

\forceindent \textit{Graphical model} is a compact representation of a collection of probability distributions. It consists of a graph $G = (V, E)$, directed or undirected, where each vertex $v \in V$ is accosiated with a random variable. Edges of the graph represent statistical relationship between nodes. There are several main types of graphical models:

* Bayesian networks
* Markov random fields
* Factor graphs

### 2.1.1 Bayesian networks

\forceindent \textit{Bayesian network} is a directed graph. Each random variable $x_i$ has an associated conditional probability distribution or local probabilistic model. A direct edge from $x_i$ to $x_j$ represents a conditional probability of a random variable, given its parents $P(x_i|x_j)$. Bayesian network defines a joint probability distribution:

$$
P(x_1, \dots, x_n) = \prod_{i=1}^{n}P(x_i\ |\ \mathbf{Par}(x_i))
\tag{2.1}
$$

### 2.1.2 Markov random fields

\forceindent \textit{Markov random fields} are based on undirected graphical models. As in a Bayesian network, nodes in a graph represent variables. An associated probability distribution factorizes into functions, each function associated with a clique of the graph:

$$
P(x_1, ..., x_n) = Z^{-1} \prod_{C \in \mathcal{C}} \psi_C(x_C)
\tag{2.2}
$$

where $Z$ is a constant chosen to ensure that the distribution is normalized. The set $\mathcal{C}$ is often taken to be the set of all maximal cliques of the graph. For a general undirected graph the compatibility functions $\psi_C$ need not have any obvious or direct relation to probabilities or conditional distributions defined over the graph cliques. 

\forceindent A special case of Markov random field is a *pairwise Markov random field* where the probability distribution factorizes into functions of two variables.

### 2.1.3 Factor graphs

\forceindent The most general parameterization is a *factor graph*. Factor graphs explicitly draw out the factors in a factorization of the joint probability. Note, that it is possible to convert arbitrary MRF or Bayesian network into equivalent factor graph.

**Defenition 2.1.** Factor graph is a pair $\mathcal{F} = (G, \{f_1, \dots, f_n\})$, where

\begin{itemize}
    \item $G = (V, E)$ is an undirected bipartite graph such that $V = X \cup F$, where $X \cup F = \emptyset$. The nodes $X$ are variable nodes, while the nodes F are factor nodes.
    \item Further, $f_1, \dots ,f_n$ are positive functions and the number of functions equals the number of nodes in $F = \{F_1, \dots , F_n\}$. Each node $F_i$ is associated with the corresponding function $f_i$ and each $f_i$ is a function of the neighboring variables of $F_i$, i.e. $f_i = f_i(\mathbf{Nb}(F_i))$.
\end{itemize}

The joint probability distribution of a factor graph of $N$ variables with $M$ functions factorizes as follows:

$$
P(\{x\}) = Z^{-1} \prod_{a=1}^M \psi(\{x\}_a)
\tag{2.3}
$$

where $Z$ is a normalization constant. 

### 2.1.3 Statistical inference

\forceindent Both directed and undirected graphical models represent a full joint probability distribution. It is important to solve the following computational inference problems:

\begin{itemize}
    \item computing the marginal distribution $P(\{x\}_A)$ over a particular subset $A \in V$ of nodes, i.e. sum over all the possible states of all the other nodes in the system
    \item computing the conditional distribution $P(\{x\}_A\ |\ \{x\}_B )$, where $A \cap B = \emptyset$ and $A, B \in V$
    \item computing the maximum a posteriori (MAP), i.e. finding the most likely joint assignment to a particular set of variables: $\text{argmax}_{\{x\}_A} P(\{x\}_A\ |\ \{x\}_B)$
\end{itemize}

**Defenition 2.2.** Marginal probabilities that are computed approximately are called *beliefs*. The belief at node $i$ is denoted by $b(x_i)$.

**Theorem 2.1.** Every type of inference in graphical models is $NP$-hard. Even simplest problem of computing the distribution over a single binary variable is $NP$-hard.

